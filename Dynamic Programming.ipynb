{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В блокноте описываются общие подходы к решению различных видов задач на динамическое программирование, встречающихся в курсовой работе по курсу \"Теория принятия решений\". А именно:\n",
    "\n",
    "- детерминированные задачи:\n",
    "  - задачи с дискретным пространством состояний (задача о рюкзаке)\n",
    "  - задачи с непрерывным пространством состояний (задача об инвестициях)\n",
    "- недетерминированные задачи (задача о садовнике)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение: динамическое программирование\n",
    "\n",
    "Итак, динамическое программирование (ДП) - это метод решения многоэтапных задач принятия решений, при котором на каждом шаге управление выбирается в соответствии с принципом Беллмана. А именно: каково бы ни было состояние системы в результате какого-то числа шагов, мы должны выбирать управление на ближайшем шаге так, чтобы оно, в совокупности с оптимальным управлением на всех последующих шагах, приводило к максимальному выигрышу на всех оставшихся шагах, включая данный.\n",
    "\n",
    "Данный принцип может быть записан с помощью уравнения Беллмана (основного функционального уравнения ДП):\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i \\in U_i(s_i)} \\{ w_i(s_i, u_i) + W_{i+1}(\\phi_i(s_i, u_i)) \\}\n",
    "$$\n",
    "\n",
    "Здесь $W_i(s_i)$ - условно оптимальный выигрыш, то есть, наилучший (максимальный) выигрыш, который может быть получен начиная с $i$-того шага, *при условии*, что система к $i$-тому шагу находится в состоянии $s_i$. $u_i$ - это управление, которое выбирается из множества допустимых управлений $U_i$. Функции $w_i(s_i, u_i)$ и $\\phi(s_i, u_i)$ задают выигрыш на $i$-том шаге и функцию изменения состояния соответственно.\n",
    "\n",
    "В различных источниках по ДП можно встретить разный \"взгляд\" на данный процесс, связанный с понятиями *cостояние* и *подзадача*. В данном случае, эти понятия обозначают одно и то же. То есть, под *состоянием* и понимается *подзадача*. В качестве примера рассмотрим задачу о рюкзаке, состоящую в том, что из имеющегося набора предметов, каждый из которых обладает определенным весом и определенной стоимостью, необходимо выбрать такое подмножество, суммарный вес которого будет не более заданного (грузоподъемность рюкзака), а стоимость максимальна. При формализации этой задачи (см. лекцию) состояние было определено как остаточная грузоподъемность рюкзака. Соответственно, факт помещения предмета в рюкзак, приводящий к уменьшению грузоподъемности, можно интерпретировать как:\n",
    "1. переход в другое состояние (с меньшей грузоподъемностью);\n",
    "2. необходимость решения меньшей задачи (с рюкзаком меньшей грузоподъемности и меньшим набором предметов).\n",
    "\n",
    "Замечание. Чтобы ДП имело смысл применять к некоторой задаче, она должна обладать определенными свойствами:\n",
    "1. оптимальная структура подзадач. Это означает, что оптимальное решение задачи достигается при оптимальных решениях подзадач. \n",
    "2. пересекающиеся подзадачи. То есть, в ходе декомпозиции исходной задачи на подзадачи одна и та же подзадача может возникать много раз.  \n",
    "\n",
    "По-хорошему, перед тем, как приступать к решению задачи методом ДП, следует (хотя бы на \"интуитивном\" уровне) убедиться в том, что задача (и ее разбиение на подзадачи) обладает данными свойствами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детерминированные задачи\n",
    "\n",
    "Особенностью детерминированных задач является то, что выигрыш от определенного управления $w_i(s_i, u_i)$ и целевое состояние при применении управления $\\phi(s_i, u_i)$ являются функциями. То есть, если мы знаем исходное состояние и управление, то выигрыш и целевое состояние определяются однозначно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи с дискретным пространством состояний\n",
    "\n",
    "В качестве примера такой задачи рассмотрим следующую (из Х. Таха Введение в исследование операций. 6-е издание):\n",
    "\n",
    "Строительный подрядчик оценивает минимальные потребности в рабочей силе на каждую из последующих пяти недель следующим образом: 5, 6, 8, 4 и 6 рабочих соответственно. Содержание избытка рабочей силы обходится подрядчику в 300 долларов за одного рабочего в неделю, а наем рабочей силы на протяжении одной недели обходится в 400 долларов плюс 200 долларов за одного рабочего в неделю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Требуемое количество рабочих для каждого этапа (каждой недели)\n",
    "workforce_demand = [5, 7, 8, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели задачи\n",
    "\n",
    "При моделировании задачи будем следовать алгоритму моделирования, изложенному в слайдах занятия, посвященного ДП (см. ). \n",
    "\n",
    "#### 1. Описание процесса\n",
    "\n",
    "*Этапы*: в данной задаче есть явная временн*а*я компонента, соответственно, логичней всего связать этапы с временем. То есть, один этап процесса управления соответствует одной неделе.\n",
    "\n",
    "*Выигрыш*: выигрыш в задаче имеет чисто экономическую основу и связывается с издержками субъекта управления. В данной задаче есть только расходы, соответственно, можно ставить задачу либо как задачу минимизации расходов, либо как максимизации выигрыша (пусть и отрицательного). Остановимся на втором варианте, чтобы не вводить дополнительное понятие \"проигрыш\".\n",
    "\n",
    "*Управление*: примем за управление изменение количества рабочих в начале недели (в точке принятия решения). Возможен и более простой вариант, в котором управлением является количество рабочих в определенную неделю.\n",
    "\n",
    "*Состояние*: количество рабочих.\n",
    "\n",
    "#### 2. Уравнение выигрыша на $i$-том этапе\n",
    "\n",
    "Выигрыш определяется всеми экономическими эффектами, связанными с управлением персоналом. А именно:\n",
    "- наем содрудников (функция $h$);\n",
    "- содержание избыточной рабочей силы (функция $m$).\n",
    "\n",
    "$$\n",
    "w_i(s_i, u_i) = h(u_i) + m(s_i + u_i - d_i).\n",
    "$$\n",
    "\n",
    "Здесь $d_i$ - это потребность в рабочих на $i$-том этапе. Обратите внимание, что при таком подходе к моделированию функция $w_i(s_i, u_i)$ оказывается зависимой от этапа (на разных этапах вызов функции с одинаковыми параметрами может дать разный результат, что объясняется различными значениями $d_i$). Чтобы отразить этот факт, в определении функции используется индекс этапа (то есть, де-факто речь идет о целом семействе функций выигрыша). \n",
    "\n",
    "Факт наема сотрудников связан с постоянными издержками (400 долларов) и переменными (200 долларов на одного рабочего). Формально это можно записать в следующем виде:\n",
    "\n",
    "$$\n",
    "h(u_i) = -(400 + 200u_i) \\mathbb{I}[u_i > 0]\n",
    "$$\n",
    "\n",
    "Здесь $\\mathbb{I}[u_i > 0]$ - индикаторная функция, принимающая значение 1 тогда и только тогда, когда выражение $u_i > 0$ истинно.\n",
    "\n",
    "Данная функция может быть определена в коде следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hire(state, control):\n",
    "    return -(400 + 200*control)*int(control > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, -2)  # Должно быть 0 (увольнения по условию задачи ничего не стоят)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, 0)  # Должно быть 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, 2)  # Должно быть -400 + (-200) + (-200) = -800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Содержание избыточной рабочей силы обходится в 300 долларов за человека:\n",
    "\n",
    "$$\n",
    "m(x) = 300 * x.\n",
    "$$\n",
    "\n",
    "Избыточной же считается рабочая сила, превышающая требуемую: $s_i + u_i - d_i$ (количество рабочих к началу этапа плюс количество нанятых рабочих минус требуемое количество)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_workers_maintenance(excess_workers):\n",
    "    return -300 * excess_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление выигрыша на заданном этапе.\n",
    "# В принципе, эта функция может еще включать в себя проверку на допустимость управления\n",
    "# (во всяком случае, при отладке эта возможность была бы весьма кстати)\n",
    "def profit(stage, state, control):\n",
    "    return hire(state, control) + excess_workers_maintenance(state + control - workforce_demand[stage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profit(0, 0, 6)   # -400 постоянные расходы на наем сотрудников,\n",
    "                  # -200*6 = -1200 переменные расходы на наем сотрудников\n",
    "                  # (-300)*(6 - 5) = -300 содержание избыточной рабочей силы\n",
    "                  # Итого: -1900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Функция перехода для  $i$-того этапа\n",
    "\n",
    "Факт найма/увольнения сотрудников изменяет количество рабочих очевидным образом:\n",
    "\n",
    "$$\n",
    "\\phi(s_i, u_i) = s_i + u_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(state, control):\n",
    "    return state + control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_transition(0, 5)  # Должно быть 5 (не было сотрудников, наняли пятерых)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Записываем уравнение Беллмана для данной задачи\n",
    "\n",
    "Объединяем все компоненты задачи:\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i + s_i \\geq d_i} \n",
    "\\{ -(400 + 200u_i) \\mathbb{I}[u_i > 0] + \n",
    "   (-300)*(s_i + u_i - d_i) + W_{i+1}(s_i + u_i) \n",
    "\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Количество месяцев\n",
    "periods = len(workforce_demand)\n",
    "# Максимальная потребность в рабочей силе\n",
    "# (используется для определения диапазона допустимых управлений)\n",
    "max_workers = max(workforce_demand)\n",
    "\n",
    "# Таблица со значениями функции Беллмана\n",
    "# Ключом является пара (этап, состояние),\n",
    "# значением - пара (условный оптимальный выигрыш, условное оптимальное управление)\n",
    "workforce_W_table = {}\n",
    "\n",
    "def workforce_W(stage, state):\n",
    "    \"\"\"Вычисление функции Беллмана для задачи планирования рабочей силы.\n",
    "       \n",
    "       Возвращаемое значение: пара (условный оптимальный выигрыш, условное оптимальное управление).\"\"\"\n",
    "\n",
    "    # Условие выхода из рекурсии\n",
    "    if stage >= periods:\n",
    "        return (0, None)\n",
    "\n",
    "    # Перед перебором управлений проверим, нет ли еще значения для данных\n",
    "    # параметров в таблице:\n",
    "    if (stage, state) in workforce_W_table:\n",
    "        return workforce_W_table[(stage, state)]\n",
    "\n",
    "    # В соответствии с уравнением Беллмана, найдем условный оптимальный \n",
    "    # выигрыш, перебирая допустимые управления и рассчитывая их эффект\n",
    "    # Диапазон, в котором имеет смысл перебирать управление, ограничен следующим\n",
    "    # образом.\n",
    "    # С одной стороны, получающееся количество сотрудников должно быть не меньше, чем\n",
    "    # требуемое на данном этапе количество.\n",
    "    # С другой стороны, количество не должно превышать максимальное требуемое количество \n",
    "    # рабочих (max_workers).\n",
    "    # В данном случае, проще перебирать не собственно управления, а состояния, в котором\n",
    "    # мы можем оказаться:\n",
    "    best_u = None\n",
    "    best_W = None\n",
    "    for workers in range(workforce_demand[stage], max_workers+1):\n",
    "        # Управление - это разница в количестве рабочих\n",
    "        control = workers - state\n",
    "        # Оценка данного управления с учетом последствий (перехода в новое состояние на следующем этапе)\n",
    "        control_evaluation = profit(stage, state, control) + workforce_W(stage+1, state_transition(state, control))[0]\n",
    "        # Если это управление лучше, чем наилучшее из известных к данному моменту - запомним его\n",
    "        if best_W is None or control_evaluation > best_W:\n",
    "            best_W = control_evaluation\n",
    "            best_u = control\n",
    "    # Сохраним найденные значения в таблицу\n",
    "    workforce_W_table[(stage, state)] = (best_W, best_u)\n",
    "    return (best_W, best_u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3300, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workforce_W(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, оптимальный план соответствует выигрышу -3300 долларов и для реализации этого плана нужно в первый месяц нанять пятерых рабочих.\n",
    "\n",
    "А как узнать условные оптимальные управления для других этапов? Они уже содержатся в таблице `workforce_W_table`, необходимо только извлечь их оттуда.\n",
    "\n",
    "Вспомним, что каждая запись таблицы отображает ключ (*номер этапа*, *количество рабочих*) в пару (*условно оптимальный выигрыш на этом и всех последующих этапах*, *условно оптимальное управление*). Это значит, что первым шагом в цепочке оптимального управления является управление 5, а второй шаг определяется тем, в каком состоянии мы окажемся в результате применения этого управления. Но для определения этого состояния мы можем воспользоваться уже имеющейся функцией перехода - `state_transition`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из состояния 0 на этапе 0 (первая неделя) под действием управления 5 мы перейдем, очевидно, в состояние 5\n",
    "state_transition(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1900, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# На этапе 1 (вторая неделя) для состояния 5 оптимальным является управление 3\n",
    "workforce_W_table[(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из состояния 5 на этапе 1 (вторая неделя) под действием управления 3 мы перейдем в состояние 8\n",
    "state_transition(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И так далее. То есть, вся последовательность оптимального управления восстанавливается с помощью моделирования переходов. Этот процесс можно обобщить с помощью следующей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 0, -2, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def restore_optimal_control(from_stage, to_stage, state):\n",
    "    \"\"\"Восстановление оптимального управления из заданного состояния на заданном этапе.\"\"\"\n",
    "    optimal_control_sequence = []\n",
    "    for stage in range(from_stage, to_stage):\n",
    "        _, control = workforce_W_table[(stage, state)]\n",
    "        # Запоминаем управление\n",
    "        optimal_control_sequence.append(control)\n",
    "        # Моделируем переход под воздействием этого управления\n",
    "        state = state_transition(state, control)\n",
    "    return optimal_control_sequence    \n",
    "\n",
    "restore_optimal_control(0, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, оптимальное управление заключается в том, чтобы на первой и второй неделях нанять по 5 и 3 рабочих соответственно, на четвертой неделе двоих уволить, а на третьей и пятой неделях ничего не менять.\n",
    "\n",
    "Очень полезно для самопроверки реализовать функцию оценки заданного управления (например, чтобы с помощью нескольких тестов убедиться в том, что найденное решение лучше многих других):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_control_sequence(from_stage, initial_state, sequence):\n",
    "    stage = from_stage\n",
    "    state = initial_state\n",
    "    cost = 0\n",
    "    for control in sequence:\n",
    "        # Эффект заданного управления в заданном состоянии\n",
    "        cost += profit(stage, state, control)\n",
    "        # Новое состояние\n",
    "        state = state_transition(state, control)\n",
    "        stage += 1\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим оценку оптимального управления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_control_sequence(0, 0, restore_optimal_control(0, 5, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, стоимость этого плана - 3300 долларов. Попробуем несколько других планов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найм максимального количества рабочих в первую же неделю\n",
    "evaluate_control_sequence(0, 0, [max_workers, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Содержание только необходимого количества рабочих в каждый месяц\n",
    "evaluate_control_sequence(0, 0, [5, 2, 1, -4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что обе тривиальные стратегии оказались хуже, чем найденная оптимальная. Конечно, в строгом смысле это не доказывает ни оптимальность стратегии, найденной методом ДП, ни корректность реализации, однако позволяет провести хотя бы базовую проверку разумности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детерминированные задачи с непрерывным состоянием\n",
    "\n",
    "См. [другой блокнот](Dynamic%20Programming%20(continuous).ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Недетерминированные задачи\n",
    "\n",
    "Особенностью таких задач является то, что при известном состоянии на $i$-том шаге $S_i$ и известном управлении $u_i$, состояние на следующем ($i+1$) шаге все равно связано с некоторой неопределенностью (как правило, раскрываемой с помощью аппарата теории вероятностей).\n",
    "\n",
    "Классическим примером такого рода задач является задача о садовнике (из Х. Таха Введение в исследование операций, 6-е издание).\n",
    "\n",
    "Каждый год в начале сезона садовник проводит химический анализ состояния почвы в своем саду. В зависимости от результатов анализа продуктивность сада на новый сезон оценивается как: 1) хорошая, 2) удовлетворительная или 3) плохая.\n",
    "\n",
    "В результате наблюдений на протяжении многих лет, садовник заметил, что продуктивность в текущем году зависит только от состояния почвы в предыдущем году. Поэтому вероятности перехода почвы из одного состояния продуктивности в другое для каждого года можно представить как следующую матрицу вероятностей перехода:\n",
    "\n",
    "$$\n",
    "P^1 = \n",
    "\\begin{pmatrix}\n",
    "0.2 & 0.5 & 0.3\\\\\n",
    "0 & 0.5 & 0.5 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "(Элемент матрицы в строке $i$ и столбце $j$ показывает вероятность перехода почвы из состояния $i$ в состояние $j$. Например, вероятность перехода почвы из состояния \"хорошая продуктивность\" (строка 1) в состояние \"плохая продуктивность\" (столбец 3) за 1 сезон оценивается как 0.3. Поскольку в рамках данной модели существует всего три возможных состояния почвы, сумма значений в каждой строке должна быть равна 1.)\n",
    "\n",
    "В результате различных агротехнических мероприятий садовник может изменить переходные вероятности $P^1$. Обычно для повышения продуктивности почвы применяются удобрения. Эти мероприятия приводят к новой матрице переходных вероятностей $P^2$:\n",
    "\n",
    "$$\n",
    "P^2 = \n",
    "\\begin{pmatrix}\n",
    "0.3 & 0.6 & 0.1\\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.05 & 0.4 & 0.55 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Чтобы рассмотреть задачу принятия решений в перспективе, садовник связывает с переходом из одного состояния почвы в другое функцию дохода (или структуру вознаграждения), которая определяет прибыль или убыток за одногодичный период в зависимости от состояний, между которыми осуществляется переход. Так как садовник может принять решение использовать или не использовать удобрения, его доход или убыток будет измениться в зависимости от принятого решения. Матрицы $R^1$ и $R^2$ определяют функции дохода (в сотнях долларов) и соответствуют матрицам переходных вероятностей $P^1$ и $P^2$:\n",
    "\n",
    "\n",
    "$$\n",
    "R^1 = \n",
    "\\begin{pmatrix}\n",
    "7 & 6 & 3 \\\\\n",
    "0 & 5 & 1 \\\\\n",
    "0 & 0 & -1 \n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = \n",
    "\\begin{pmatrix}\n",
    "6 & 5 & -1 \\\\\n",
    "7 & 4 & 0 \\\\\n",
    "6 & 3 & -2 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Элементы $r_{ij}^2$ матрицы $R^2$ учитывают затраты, связанные с применением удобрения. Например, если система находится в состоянии 1 и остается в этом состоянии и в следующем году, то доход составит $r_{11}^2 = 6$, если же удобрения не используются, то $r_{11}^1 = 7$. \n",
    "\n",
    "В конечном итоге, садовник хочет выработать стратегию поведения (вносить удобрения или не вносить), позволяющую добиться *максимального дохода*.\n",
    "\n",
    "Поскольку конкретное значение дохода от реализуемой стратегии внесения удобрений характеризуется неопределенностью, речь идет о максимизации *ожидаемого дохода* (математического ожидания дохода). (Эта подмена, в частности, является следствием применения теории полезности фон Неймана - Моргенштерна, одним из наиболее распространенных способов сравнивать полезность событий, обладающих разными вероятностями.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи \"от базовых принципов\"\n",
    "\n",
    "Рассмотрим вариант задачи о садовнике, в котором ставится задача максимизации ожидаемого дохода за ограниченный период времени $N$ лет. То есть, садовник хочет выработать такую стратегию внесения удобрений, при которой его ожидаемый доход за определенный период был бы максимален. (При этом вполне возможно, что к концу периода почва окажется в очень \"невыгодном\" состоянии.)\n",
    "\n",
    "Оказывается, что для решения такой задачи вполне подходит принцип ДП. Однако для учета неопределенности переходов в модель ДП, рассмотренную выше, нужно ввести несколько уточнений.\n",
    "\n",
    "Пусть $s_i \\in S_i$ - состояние почвы на $i$-том этапе (в $i$-тый год), а $u_i$ - управление, применяемое в $i$-тый год ($u_i = 1$ означает, что удобрения не вносятся, а $u_i = 2$ означает, что вносятся).\n",
    "\n",
    "Уточнение 1. В детерминированных задачах переход из некоторого состояния $s_i$ на $i$-том этапе под действием управления $u_i$ задавался *функцией* $\\phi(s_i, u_i)$. То есть, при заданном исходном состоянии и заданном управлении целевое состояние можно было определить однозначно. В данном случае это не так. При любом управлении потенциально может быть возможен переход из любого состояния в любоей (возможно, с разными вероятностями). Поэтому вместо функции $\\phi(s_i, u_i)$ имеет смысл рассматривать функцию вероятности перехода $\\phi(s_i, s_{i+1}, u_i)$ (по сути, определяемую матрицей перехода вероятностей $P^{u_i}$).\n",
    "\n",
    "Уточнение 2. В детерминированных задачах выигрыш в состоянии $s_i$ под действием управления $u_i$ также задавался *функцией* $w(s_i, u_i)$. В данной задаче и это не так. Выигрыш зависит не только от того, какое управление мы применили, но и в какое состояние систему \"занесло\" в силу воздействия совокупности факторов, оценить которые мы можем только вероятностно. То есть, на смену функции выигрыша $w(s_i, u_i)$ приходит функция выигрыша $w(s_i, s_{i+1}, u_i)$ (по сути, это матрица стоимостей переходов $R^{u_i}$).\n",
    "\n",
    "Уточнение 3. Ожидаемый выигрыш при условии, что система к $i$-тому этапу находится в состоянии $S_i$, также имеет рекуррентную природу, но уравнение Беллмана записывается уже для максимизации математического ожидания:\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i \\in U_i(s_i)} \\{ \\sum_{s_j \\in S_{i+1}} \\phi_i(s_i, s_j, u_i) (w_i(s_i, s_j, u_i) + W_{i+1}(s_j)) \\}\n",
    "$$\n",
    "\n",
    "То есть, условный оптимальный ожидаемый выигрыш на всех этапах, начиная с $i$-того, при условии, что $i$-тому этапу система находится в состоянии $s_i$, определяется как выбор такого управления $u_i$ из множества допустимых управлений $U_i(s_i)$, при котором максимизируется математическое ожидание суммы стоимости перехода и ожидаемого условного оптимального выигрыша на последующих этапах. Здесь $S_{i+1}$ - множество тех состояний, в которых система может оказаться на этапе $i+1$ (в задаче о садовнике множество состояний одинаково на всех этапах, но это может быть и не так).\n",
    "\n",
    "Модифицируем программную реализацию функции Беллмана для максимизации ожидаемого выигрыша. В первую очередь, зададим матрицы, описывающие вероятность и стоимости переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Матрицы переходов из условия задачи\n",
    "P1 = np.array([[0.2, 0.5, 0.3],\n",
    "               [  0, 0.5, 0.5],\n",
    "               [  0,   0,   1]])\n",
    "\n",
    "P2 = np.array([[ 0.3, 0.6,  0.1],\n",
    "               [ 0.1, 0.6,  0.3],\n",
    "               [0.05, 0.4, 0.55]])\n",
    "\n",
    "def assert_transition_probabilities(m):\n",
    "    \"\"\"Проверяет, является ли заданная матрица правильной матрицей вероятностей переходов.\"\"\"\n",
    "    if np.any(abs(m.sum(axis=1) - 1) > 1e-9):\n",
    "        raise Exception('Each row has to sum to 1.')\n",
    "\n",
    "assert_transition_probabilities(P1)\n",
    "assert_transition_probabilities(P2)\n",
    "\n",
    "# Матрицы стоимостей переходов из условия задачи\n",
    "\n",
    "R1 = np.array([[7, 6,  3],\n",
    "               [0, 5,  1],\n",
    "               [0, 0, -1]])\n",
    "R2 = np.array([[6, 5, -1],\n",
    "               [7, 4,  0],\n",
    "               [6, 3, -2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы реализация вычисления функции Беллмана была максимально похожа на реализацию для детерминированного случая, определим функции выигрыша и вероятности перехода. В принципе, этого можно и не делать и работать в функции Беллмана напрямую с матрицами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(state_from, state_to, control):\n",
    "    if control == 1:\n",
    "        return R1[state_from, state_to]\n",
    "    elif control == 2:\n",
    "        return R2[state_from, state_to]\n",
    "    else:\n",
    "        raise Exception(f'Invalid control: {control}')\n",
    "        \n",
    "def state_transition_probability(state_from, state_to, control):\n",
    "    if control == 1:\n",
    "        return P1[state_from, state_to]\n",
    "    elif control == 2:\n",
    "        return P2[state_from, state_to]\n",
    "    else:\n",
    "        raise Exception(f'Invalid control: {control}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, вычисление функции Беллмана:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество лет (горизонт планирования)\n",
    "gardener_periods = 3\n",
    "\n",
    "# Таблица со значениями функции Беллмана\n",
    "# Ключом является пара (этап, состояние),\n",
    "# значением - пара (ожидаемый условный оптимальный выигрыш, условное оптимальное управление)\n",
    "gardener_W_table = {}\n",
    "\n",
    "def gardener_W(stage, state):\n",
    "    \"\"\"Вычисление функции Беллмана для задачи о садовнике.\n",
    "       \n",
    "       Возвращаемое значение: пара (ожидаемый условный оптимальный выигрыш, условное оптимальное управление).\"\"\"\n",
    "\n",
    "    # Условие выхода из рекурсии\n",
    "    if stage >= gardener_periods:\n",
    "        return (0, None)\n",
    "\n",
    "    # Перед перебором управлений проверим, нет ли еще значения для данных\n",
    "    # параметров в таблице:\n",
    "    if (stage, state) in gardener_W_table:\n",
    "        return gardener_W_table[(stage, state)]\n",
    "\n",
    "    # В соответствии с уравнением Беллмана для вероятностных задач, найдем \n",
    "    # ожидаемый условный оптимальный выигрыш, перебирая допустимые управления,\n",
    "    # и рассчитывая математическое ожидание их эффекта с учетом вероятностей переходов.\n",
    "    best_u = None\n",
    "    best_W = None\n",
    "    for control in [1, 2]:\n",
    "        # 1 - не вносить удобрения, 2 - вносить\n",
    "        \n",
    "        # Потенциально, при любом управлении мы можем перейти в любое\n",
    "        # состояние, поэтому здесь добавляется еще один цикл для оценки\n",
    "        # математического ожидания:\n",
    "        accumulated_sum = 0\n",
    "        for new_state in range(3):\n",
    "            accumulated_sum += state_transition_probability(state, new_state, control) * \\\n",
    "                                    (profit(state, new_state, control) + gardener_W(stage+1, new_state)[0])\n",
    "\n",
    "        # Если это управление лучше, чем наилучшее из известных к данному моменту - запомним его\n",
    "        if best_W is None or accumulated_sum > best_W:\n",
    "            best_W = accumulated_sum\n",
    "            best_u = control\n",
    "            \n",
    "    # Сохраним найденные значения в таблицу\n",
    "    workforce_W_table[(stage, state)] = (best_W, best_u)\n",
    "    return (best_W, best_u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим оценку ожидаемого условного оптимального выигрыша (если в начале процесса почва была в хорошем состоянии):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.7355, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_W(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, в этом случае наибольший ожидаемый выигрыш будет равен приблизительно 10.74 сотен долларов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Восстановление оптимальной стратегии\n",
    "\n",
    "В детерминированной модели мы могли полностью восстановить всю цепочку управлений (поскольку достоверно знали в каком состоянии система окажется в результате каждого из них). Здесь так сделать не получится - действительно, после того, как садовник применил оптимальное управление в первый год (осуществил подкормку), почва могла перейти в любое из трех состояний, и заранее сказать каким должно быть управление во второй год мы уже не можем.\n",
    "\n",
    "Поэтому для подобных задач результатом является полная *стратегия* оптимального управления, для каждого этапа и каждого состояния определяющая оптимальное действие (действие, приводящее к наибольшему ожидаемому выигрышу).\n",
    "\n",
    "Как и в предыдущем случае, данная стратегия уже содержится в словаре `gardener_W_table`, необходимо только извлечь ее оттуда. Для этого можно воспользоваться как самим словарем, так и функцией `gardener_W` которая все равно не будет рассчитывать значений, поскольку они уже содержатся в словаре. Итак, оптимальная стратегия для первого года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 1-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Применять удобрения\" -> 10.7355\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 7.922499999999999\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 4.22225\n"
     ]
    }
   ],
   "source": [
    "state_decoder = ['хорошее', 'удовлетворительное', 'плохое']\n",
    "control_decoder = [None, 'Не применять удобрения', 'Применять удобрения']\n",
    "\n",
    "print('Стратегия для 1-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(0, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично находится стратегия для 2-го года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 2-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Применять удобрения\" -> 8.19\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 5.61\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 2.125\n"
     ]
    }
   ],
   "source": [
    "print('Стратегия для 2-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(1, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И для третьего года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 3-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Не применять удобрения\" -> 5.300000000000001\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 3.1\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 0.40000000000000013\n"
     ]
    }
   ],
   "source": [
    "print('Стратегия для 3-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(2, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование стратегии\n",
    "\n",
    "В качестве самопроверки может оказаться полезным промоделировать ту или иную стратегию управления и определить ожидаемый выигрыш от следования ей.\n",
    "\n",
    "Процесс может быть организован точно так же, как и вычисление функции Беллмана, только на каждом этапе нет необходимости осуществлять оптимизацию (выбор управления), требуется просто применять заданное управление.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внимание! Данная функция очень неэффективна без мемоизации. Лучше делать то же самое, но\n",
    "# в обратном порядке и без рекурсии\n",
    "def gardener_eval_strategy(strategy, state):\n",
    "    if len(strategy) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Управление, диктуемое текущей стратегией для данного состояния\n",
    "    control = strategy[0][state]\n",
    "    \n",
    "    accumulated_cost = 0\n",
    "    for new_state in range(3):\n",
    "        accumulated_cost += state_transition_probability(state, new_state, control) * \\\n",
    "                                    (profit(state, new_state, control) + gardener_eval_strategy(strategy[1:], new_state))\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим эффект \"скупой\" стратегии, при которой удобрения не вносятся ни при каких условиях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.212000000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_eval_strategy([[1, 1, 1],   # Управление для каждого из состояний на первом этапе\n",
    "                        [1, 1, 1],   # Управление для каждого из состояний на втором этапе\n",
    "                        [1, 1, 1]],  # ...\n",
    "                       0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, даже если мы начнем с почвы в \"хорошем\" состоянии, ожидаемый выигрыш будет около 8.2.\n",
    "\n",
    "Оценим эффект \"расточительной\" стратегии, при которой удобрения вносятся безусловно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.6425"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_eval_strategy([[2, 2, 2],\n",
    "                        [2, 2, 2],\n",
    "                        [2, 2, 2]],\n",
    "                       0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что при данной стратегии ожидаемый выигрыш оказывается очень близким к оптимальному. Это вполне ожидаемо, учитывая, что оптимальная стратегия почти совпадает с данной (единственное отличие заключается в том, что в оптимальной стратегии на третьем году в хорошую почву удобрения не вносятся)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матричная реализация\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сведение задачи к марковскому процессу принятия решений\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
