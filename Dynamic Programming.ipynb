{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В блокноте описываются общие подходы к решению различных видов задач на динамическое программирование, встречающихся в курсовой работе по курсу \"Теория принятия решений\". А именно:\n",
    "\n",
    "- детерминированные задачи:\n",
    "  - задачи с дискретным пространством состояний (задача о рюкзаке)\n",
    "  - задачи с непрерывным пространством состояний (задача об инвестициях)\n",
    "- недетерминированные задачи (задача о садовнике)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение: динамическое программирование\n",
    "\n",
    "Итак, динамическое программирование (ДП) - это метод решения многоэтапных задач принятия решений, при котором на каждом шаге управление выбирается в соответствии с принципом Беллмана. А именно: каково бы ни было состояние системы в результате какого-то числа шагов, мы должны выбирать управление на ближайшем шаге так, чтобы оно, в совокупности с оптимальным управлением на всех последующих шагах, приводило к максимальному выигрышу на всех оставшихся шагах, включая данный.\n",
    "\n",
    "Данный принцип может быть записан с помощью уравнения Беллмана (основного функционального уравнения ДП):\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i \\in U_i(s_i)} \\{ w_i(s_i, u_i) + W_{i+1}(\\phi_i(s_i, u_i)) \\}\n",
    "$$\n",
    "\n",
    "Здесь $W_i(s_i)$ - условно оптимальный выигрыш, то есть, наилучший (максимальный) выигрыш, который может быть получен начиная с $i$-того шага, *при условии*, что система к $i$-тому шагу находится в состоянии $s_i$. $u_i$ - это управление, которое выбирается из множества допустимых управлений $U_i$. Функции $w_i(s_i, u_i)$ и $\\phi(s_i, u_i)$ задают выигрыш на $i$-том шаге и функцию изменения состояния соответственно.\n",
    "\n",
    "В различных источниках по ДП можно встретить разный \"взгляд\" на данный процесс, связанный с понятиями *cостояние* и *подзадача*. В данном случае, эти понятия обозначают одно и то же. То есть, под *состоянием* и понимается *подзадача*. В качестве примера рассмотрим задачу о рюкзаке, состоящую в том, что из имеющегося набора предметов, каждый из которых обладает определенным весом и определенной стоимостью, необходимо выбрать такое подмножество, суммарный вес которого будет не более заданного (грузоподъемность рюкзака), а стоимость максимальна. При формализации этой задачи (см. лекцию) состояние было определено как остаточная грузоподъемность рюкзака. Соответственно, факт помещения предмета в рюкзак, приводящий к уменьшению грузоподъемности, можно интерпретировать как:\n",
    "1. переход в другое состояние (с меньшей грузоподъемностью);\n",
    "2. необходимость решения меньшей задачи (с рюкзаком меньшей грузоподъемности и меньшим набором предметов).\n",
    "\n",
    "Замечание. Чтобы ДП имело смысл применять к некоторой задаче, она должна обладать определенными свойствами:\n",
    "1. оптимальная структура подзадач. Это означает, что оптимальное решение задачи достигается при оптимальных решениях подзадач. \n",
    "2. пересекающиеся подзадачи. То есть, в ходе декомпозиции исходной задачи на подзадачи одна и та же подзадача может возникать много раз.  \n",
    "\n",
    "По-хорошему, перед тем, как приступать к решению задачи методом ДП, следует (хотя бы на \"интуитивном\" уровне) убедиться в том, что задача (и ее разбиение на подзадачи) обладает данными свойствами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детерминированные задачи\n",
    "\n",
    "Особенностью детерминированных задач является то, что выигрыш от определенного управления $w_i(s_i, u_i)$ и целевое состояние при применении управления $\\phi(s_i, u_i)$ являются функциями. То есть, если мы знаем исходное состояние и управление, то выигрыш и целевое состояние определяются однозначно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи с дискретным пространством состояний\n",
    "\n",
    "В качестве примера такой задачи рассмотрим следующую (из Х. Таха Введение в исследование операций. 6-е издание):\n",
    "\n",
    "Строительный подрядчик оценивает минимальные потребности в рабочей силе на каждую из последующих пяти недель следующим образом: 5, 6, 8, 4 и 6 рабочих соответственно. Содержание избытка рабочей силы обходится подрядчику в 300 долларов за одного рабочего в неделю, а наем рабочей силы на протяжении одной недели обходится в 400 долларов плюс 200 долларов за одного рабочего в неделю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Требуемое количество рабочих для каждого этапа (каждой недели)\n",
    "workforce_demand = [5, 7, 8, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели задачи\n",
    "\n",
    "При моделировании задачи будем следовать алгоритму моделирования, изложенному в слайдах занятия, посвященного ДП (см. ). \n",
    "\n",
    "#### 1. Описание процесса\n",
    "\n",
    "*Этапы*: в данной задаче есть явная временн*а*я компонента, соответственно, логичней всего связать этапы с временем. То есть, один этап процесса управления соответствует одной неделе.\n",
    "\n",
    "*Выигрыш*: выигрыш в задаче имеет чисто экономическую основу и связывается с издержками субъекта управления. В данной задаче есть только расходы, соответственно, можно ставить задачу либо как задачу минимизации расходов, либо как максимизации выигрыша (пусть и отрицательного). Остановимся на втором варианте, чтобы не вводить дополнительное понятие \"проигрыш\".\n",
    "\n",
    "*Управление*: примем за управление изменение количества рабочих в начале недели (в точке принятия решения). Возможен и более простой вариант, в котором управлением является количество рабочих в определенную неделю.\n",
    "\n",
    "*Состояние*: количество рабочих.\n",
    "\n",
    "#### 2. Уравнение выигрыша на $i$-том этапе\n",
    "\n",
    "Выигрыш определяется всеми экономическими эффектами, связанными с управлением персоналом. А именно:\n",
    "- наем содрудников (функция $h$);\n",
    "- содержание избыточной рабочей силы (функция $m$).\n",
    "\n",
    "$$\n",
    "w_i(s_i, u_i) = h(u_i) + m(s_i + u_i - d_i).\n",
    "$$\n",
    "\n",
    "Здесь $d_i$ - это потребность в рабочих на $i$-том этапе. Обратите внимание, что при таком подходе к моделированию функция $w_i(s_i, u_i)$ оказывается зависимой от этапа (на разных этапах вызов функции с одинаковыми параметрами может дать разный результат, что объясняется различными значениями $d_i$). Чтобы отразить этот факт, в определении функции используется индекс этапа (то есть, де-факто речь идет о целом семействе функций выигрыша). \n",
    "\n",
    "Факт наема сотрудников связан с постоянными издержками (400 долларов) и переменными (200 долларов на одного рабочего). Формально это можно записать в следующем виде:\n",
    "\n",
    "$$\n",
    "h(u_i) = -(400 + 200u_i) \\mathbb{I}[u_i > 0]\n",
    "$$\n",
    "\n",
    "Здесь $\\mathbb{I}[u_i > 0]$ - индикаторная функция, принимающая значение 1 тогда и только тогда, когда выражение $u_i > 0$ истинно.\n",
    "\n",
    "Данная функция может быть определена в коде следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hire(state, control):\n",
    "    return -(400 + 200*control)*int(control > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, -2)  # Должно быть 0 (увольнения по условию задачи ничего не стоят)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, 0)  # Должно быть 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hire(None, 2)  # Должно быть -400 + (-200) + (-200) = -800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Содержание избыточной рабочей силы обходится в 300 долларов за человека:\n",
    "\n",
    "$$\n",
    "m(x) = 300 * x.\n",
    "$$\n",
    "\n",
    "Избыточной же считается рабочая сила, превышающая требуемую: $s_i + u_i - d_i$ (количество рабочих к началу этапа плюс количество нанятых рабочих минус требуемое количество)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_workers_maintenance(excess_workers):\n",
    "    return -300 * excess_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление выигрыша на заданном этапе.\n",
    "# В принципе, эта функция может еще включать в себя проверку на допустимость управления\n",
    "# (во всяком случае, при отладке эта возможность была бы весьма кстати)\n",
    "def profit(stage, state, control):\n",
    "    return hire(state, control) + excess_workers_maintenance(state + control - workforce_demand[stage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profit(0, 0, 6)   # -400 постоянные расходы на наем сотрудников,\n",
    "                  # -200*6 = -1200 переменные расходы на наем сотрудников\n",
    "                  # (-300)*(6 - 5) = -300 содержание избыточной рабочей силы\n",
    "                  # Итого: -1900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Функция перехода для  $i$-того этапа\n",
    "\n",
    "Факт найма/увольнения сотрудников изменяет количество рабочих очевидным образом:\n",
    "\n",
    "$$\n",
    "\\phi(s_i, u_i) = s_i + u_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(state, control):\n",
    "    return state + control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_transition(0, 5)  # Должно быть 5 (не было сотрудников, наняли пятерых)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Записываем уравнение Беллмана для данной задачи\n",
    "\n",
    "Объединяем все компоненты задачи:\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i + s_i \\geq d_i} \n",
    "\\{ -(400 + 200u_i) \\mathbb{I}[u_i > 0] + \n",
    "   (-300)*(s_i + u_i - d_i) + W_{i+1}(s_i + u_i) \n",
    "\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Количество месяцев\n",
    "periods = len(workforce_demand)\n",
    "# Максимальная потребность в рабочей силе\n",
    "# (используется для определения диапазона допустимых управлений)\n",
    "max_workers = max(workforce_demand)\n",
    "\n",
    "# Таблица со значениями функции Беллмана\n",
    "# Ключом является пара (этап, состояние),\n",
    "# значением - пара (условный оптимальный выигрыш, условное оптимальное управление)\n",
    "workforce_W_table = {}\n",
    "\n",
    "def workforce_W(stage, state):\n",
    "    \"\"\"Вычисление функции Беллмана для задачи планирования рабочей силы.\n",
    "       \n",
    "       Возвращаемое значение: пара (условный оптимальный выигрыш, условное оптимальное управление).\"\"\"\n",
    "\n",
    "    # Условие выхода из рекурсии\n",
    "    if stage >= periods:\n",
    "        return (0, None)\n",
    "\n",
    "    # Перед перебором управлений проверим, нет ли еще значения для данных\n",
    "    # параметров в таблице:\n",
    "    if (stage, state) in workforce_W_table:\n",
    "        return workforce_W_table[(stage, state)]\n",
    "\n",
    "    # В соответствии с уравнением Беллмана, найдем условный оптимальный \n",
    "    # выигрыш, перебирая допустимые управления и рассчитывая их эффект\n",
    "    # Диапазон, в котором имеет смысл перебирать управление, ограничен следующим\n",
    "    # образом.\n",
    "    # С одной стороны, получающееся количество сотрудников должно быть не меньше, чем\n",
    "    # требуемое на данном этапе количество.\n",
    "    # С другой стороны, количество не должно превышать максимальное требуемое количество \n",
    "    # рабочих (max_workers).\n",
    "    # В данном случае, проще перебирать не собственно управления, а состояния, в котором\n",
    "    # мы можем оказаться:\n",
    "    best_u = None\n",
    "    best_W = None\n",
    "    for workers in range(workforce_demand[stage], max_workers+1):\n",
    "        # Управление - это разница в количестве рабочих\n",
    "        control = workers - state\n",
    "        # Оценка данного управления с учетом последствий (перехода в новое состояние на следующем этапе)\n",
    "        control_evaluation = profit(stage, state, control) + workforce_W(stage+1, state_transition(state, control))[0]\n",
    "        # Если это управление лучше, чем наилучшее из известных к данному моменту - запомним его\n",
    "        if best_W is None or control_evaluation > best_W:\n",
    "            best_W = control_evaluation\n",
    "            best_u = control\n",
    "    # Сохраним найденные значения в таблицу\n",
    "    workforce_W_table[(stage, state)] = (best_W, best_u)\n",
    "    return (best_W, best_u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3300, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workforce_W(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, оптимальный план соответствует выигрышу -3300 долларов и для реализации этого плана нужно в первый месяц нанять пятерых рабочих.\n",
    "\n",
    "А как узнать условные оптимальные управления для других этапов? Они уже содержатся в таблице `workforce_W_table`, необходимо только извлечь их оттуда.\n",
    "\n",
    "Вспомним, что каждая запись таблицы отображает ключ (*номер этапа*, *количество рабочих*) в пару (*условно оптимальный выигрыш на этом и всех последующих этапах*, *условно оптимальное управление*). Это значит, что первым шагом в цепочке оптимального управления является управление 5, а второй шаг определяется тем, в каком состоянии мы окажемся в результате применения этого управления. Но для определения этого состояния мы можем воспользоваться уже имеющейся функцией перехода - `state_transition`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из состояния 0 на этапе 0 (первая неделя) под действием управления 5 мы перейдем, очевидно, в состояние 5\n",
    "state_transition(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1900, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# На этапе 1 (вторая неделя) для состояния 5 оптимальным является управление 3\n",
    "workforce_W_table[(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из состояния 5 на этапе 1 (вторая неделя) под действием управления 3 мы перейдем в состояние 8\n",
    "state_transition(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И так далее. То есть, вся последовательность оптимального управления восстанавливается с помощью моделирования переходов. Этот процесс можно обобщить с помощью следующей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 0, -2, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def restore_optimal_control(from_stage, to_stage, state):\n",
    "    \"\"\"Восстановление оптимального управления из заданного состояния на заданном этапе.\"\"\"\n",
    "    optimal_control_sequence = []\n",
    "    for stage in range(from_stage, to_stage):\n",
    "        _, control = workforce_W_table[(stage, state)]\n",
    "        # Запоминаем управление\n",
    "        optimal_control_sequence.append(control)\n",
    "        # Моделируем переход под воздействием этого управления\n",
    "        state = state_transition(state, control)\n",
    "    return optimal_control_sequence    \n",
    "\n",
    "restore_optimal_control(0, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, оптимальное управление заключается в том, чтобы на первой и второй неделях нанять по 5 и 3 рабочих соответственно, на четвертой неделе двоих уволить, а на третьей и пятой неделях ничего не менять.\n",
    "\n",
    "Очень полезно для самопроверки реализовать функцию оценки заданного управления (например, чтобы с помощью нескольких тестов убедиться в том, что найденное решение лучше многих других):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_control_sequence(from_stage, initial_state, sequence):\n",
    "    stage = from_stage\n",
    "    state = initial_state\n",
    "    cost = 0\n",
    "    for control in sequence:\n",
    "        # Эффект заданного управления в заданном состоянии\n",
    "        cost += profit(stage, state, control)\n",
    "        # Новое состояние\n",
    "        state = state_transition(state, control)\n",
    "        stage += 1\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим оценку оптимального управления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_control_sequence(0, 0, restore_optimal_control(0, 5, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, стоимость этого плана - 3300 долларов. Попробуем несколько других планов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найм максимального количества рабочих в первую же неделю\n",
    "evaluate_control_sequence(0, 0, [max_workers, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Содержание только необходимого количества рабочих в каждый месяц\n",
    "evaluate_control_sequence(0, 0, [5, 2, 1, -4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что обе тривиальные стратегии оказались хуже, чем найденная оптимальная. Конечно, в строгом смысле это не доказывает ни оптимальность стратегии, найденной методом ДП, ни корректность реализации, однако позволяет провести хотя бы базовую проверку разумности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детерминированные задачи с непрерывным состоянием\n",
    "\n",
    "См. [другой блокнот](Dynamic%20Programming%20(continuous).ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Недетерминированные задачи\n",
    "\n",
    "Особенностью таких задач является то, что при известном состоянии на $i$-том шаге $S_i$ и известном управлении $u_i$, состояние на следующем ($i+1$) шаге, тем не менее, связано с некоторой неопределенностью (как правило, раскрываемой с помощью аппарата теории вероятностей).\n",
    "\n",
    "Классическим примером такого рода задач является задача о садовнике (из Х. Таха Введение в исследование операций, 6-е издание).\n",
    "\n",
    "Каждый год в начале сезона садовник проводит химический анализ состояния почвы в своем саду. В зависимости от результатов анализа продуктивность сада на новый сезон оценивается как: 1) хорошая, 2) удовлетворительная или 3) плохая.\n",
    "\n",
    "В результате наблюдений на протяжении многих лет, садовник заметил, что продуктивность в текущем году зависит только от состояния почвы в предыдущем году. Поэтому вероятности перехода почвы из одного состояния продуктивности в другое для каждого года можно представить как следующую матрицу вероятностей перехода:\n",
    "\n",
    "$$\n",
    "P^1 = \n",
    "\\begin{pmatrix}\n",
    "0.2 & 0.5 & 0.3\\\\\n",
    "0 & 0.5 & 0.5 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "(Элемент матрицы в строке $i$ и столбце $j$ показывает вероятность перехода почвы из состояния $i$ в состояние $j$. Например, вероятность перехода почвы из состояния \"хорошая продуктивность\" (строка 1) в состояние \"плохая продуктивность\" (столбец 3) за 1 сезон оценивается как 0.3. Поскольку в рамках данной модели существует всего три возможных состояния почвы, сумма значений в каждой строке должна быть равна 1.)\n",
    "\n",
    "В результате различных агротехнических мероприятий садовник может изменить переходные вероятности $P^1$. Обычно для повышения продуктивности почвы применяются удобрения. Эти мероприятия приводят к новой матрице переходных вероятностей $P^2$:\n",
    "\n",
    "$$\n",
    "P^2 = \n",
    "\\begin{pmatrix}\n",
    "0.3 & 0.6 & 0.1\\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.05 & 0.4 & 0.55 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Чтобы рассмотреть задачу принятия решений в перспективе, садовник связывает с переходом из одного состояния почвы в другое функцию дохода (или структуру вознаграждения), которая определяет прибыль или убыток за одногодичный период в зависимости от состояний, между которыми осуществляется переход. Так как садовник может принять решение использовать или не использовать удобрения, его доход или убыток будет измениться в зависимости от принятого решения. Матрицы $R^1$ и $R^2$ определяют функции дохода (в сотнях долларов) и соответствуют матрицам переходных вероятностей $P^1$ и $P^2$:\n",
    "\n",
    "\n",
    "$$\n",
    "R^1 = \n",
    "\\begin{pmatrix}\n",
    "7 & 6 & 3 \\\\\n",
    "0 & 5 & 1 \\\\\n",
    "0 & 0 & -1 \n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = \n",
    "\\begin{pmatrix}\n",
    "6 & 5 & -1 \\\\\n",
    "7 & 4 & 0 \\\\\n",
    "6 & 3 & -2 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Элементы $r_{ij}^2$ матрицы $R^2$ учитывают затраты, связанные с применением удобрения. Например, если система находится в состоянии 1 и остается в этом состоянии и в следующем году, то доход составит $r_{11}^2 = 6$, если же удобрения не используются, то $r_{11}^1 = 7$. \n",
    "\n",
    "В конечном итоге, садовник хочет выработать стратегию поведения (вносить удобрения или не вносить), позволяющую добиться *максимального дохода*.\n",
    "\n",
    "Поскольку конкретное значение дохода от реализуемой стратегии внесения удобрений характеризуется неопределенностью, речь идет о максимизации *ожидаемого дохода* (математического ожидания дохода). (Эта подмена, в частности, является следствием применения теории полезности фон Неймана - Моргенштерна, одним из наиболее распространенных способов сравнивать полезность событий, обладающих разными вероятностями.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи \"от базовых принципов\"\n",
    "\n",
    "Рассмотрим вариант задачи о садовнике, в котором ставится задача максимизации ожидаемого дохода за ограниченный период времени $N$ лет. То есть, садовник хочет выработать такую стратегию внесения удобрений, при которой его ожидаемый доход за определенный период был бы максимален. (При этом вполне возможно, что к концу периода почва окажется в очень \"невыгодном\" состоянии.)\n",
    "\n",
    "Оказывается, что для решения такой задачи вполне подходит принцип ДП. Однако для учета неопределенности переходов в модель ДП, рассмотренную выше, нужно ввести несколько уточнений.\n",
    "\n",
    "Пусть $s_i \\in S_i$ - состояние почвы на $i$-том этапе (в $i$-тый год), а $u_i$ - управление, применяемое в $i$-тый год ($u_i = 1$ означает, что удобрения не вносятся, а $u_i = 2$ означает, что вносятся).\n",
    "\n",
    "Уточнение 1. В детерминированных задачах переход из некоторого состояния $s_i$ на $i$-том этапе под действием управления $u_i$ задавался *функцией* $\\phi(s_i, u_i)$. То есть, при заданном исходном состоянии и заданном управлении целевое состояние можно было определить однозначно. В данном случае это не так. При любом управлении потенциально возможен переход из любого состояния в любое (с разными вероятностями). Поэтому вместо функции $\\phi(s_i, u_i)$ имеет смысл рассматривать функцию вероятности перехода $\\phi(s_i, s_{i+1}, u_i)$ (по сути, определяемую матрицей перехода вероятностей $P^{u_i}$).\n",
    "\n",
    "Уточнение 2. В детерминированных задачах выигрыш в состоянии $s_i$ под действием управления $u_i$ также задавался *функцией* $w(s_i, u_i)$. В данной задаче и это не так. Выигрыш зависит не только от того, какое управление мы применили, но и в какое состояние систему \"занесло\" в силу воздействия совокупности факторов, оценить которые мы можем только вероятностно. То есть, на смену функции выигрыша $w(s_i, u_i)$ приходит функция выигрыша $w(s_i, s_{i+1}, u_i)$ (по сути, это матрица стоимостей переходов $R^{u_i}$).\n",
    "\n",
    "Уточнение 3. Ожидаемый выигрыш при условии, что система к $i$-тому этапу находится в состоянии $S_i$, также имеет рекуррентную природу, но уравнение Беллмана записывается уже для максимизации математического ожидания:\n",
    "\n",
    "$$\n",
    "W_i(s_i) = \\max_{u_i \\in U_i(s_i)} \\{ \\sum_{s_j \\in S_{i+1}} \\phi_i(s_i, s_j, u_i) (w_i(s_i, s_j, u_i) + W_{i+1}(s_j)) \\}\n",
    "$$\n",
    "\n",
    "То есть, ожидаемый условный оптимальный выигрыш на всех этапах, начиная с $i$-того, при условии, что $i$-тому этапу система находится в состоянии $s_i$, определяется как выбор такого управления $u_i$ из множества допустимых управлений $U_i(s_i)$, при котором максимизируется математическое ожидание суммы стоимости перехода и ожидаемого условного оптимального выигрыша на последующих этапах. Здесь $S_{i+1}$ - множество тех состояний, в которых система может оказаться на этапе $i+1$ (в задаче о садовнике множество состояний одинаково на всех этапах, но это может быть и не так).\n",
    "\n",
    "Модифицируем программную реализацию функции Беллмана для максимизации ожидаемого выигрыша. В первую очередь, зададим матрицы, описывающие вероятность и стоимости переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Матрицы переходов из условия задачи\n",
    "P1 = np.array([[0.2, 0.5, 0.3],\n",
    "               [  0, 0.5, 0.5],\n",
    "               [  0,   0,   1]])\n",
    "\n",
    "P2 = np.array([[ 0.3, 0.6,  0.1],\n",
    "               [ 0.1, 0.6,  0.3],\n",
    "               [0.05, 0.4, 0.55]])\n",
    "\n",
    "def assert_transition_probabilities(m):\n",
    "    \"\"\"Проверяет, является ли заданная матрица правильной матрицей вероятностей переходов.\"\"\"\n",
    "    if np.any(abs(m.sum(axis=1) - 1) > 1e-9):\n",
    "        raise Exception('Each row has to sum to 1.')\n",
    "\n",
    "assert_transition_probabilities(P1)\n",
    "assert_transition_probabilities(P2)\n",
    "\n",
    "# Матрицы стоимостей переходов из условия задачи\n",
    "\n",
    "R1 = np.array([[7, 6,  3],\n",
    "               [0, 5,  1],\n",
    "               [0, 0, -1]])\n",
    "R2 = np.array([[6, 5, -1],\n",
    "               [7, 4,  0],\n",
    "               [6, 3, -2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы реализация вычисления функции Беллмана была максимально похожа на реализацию для детерминированного случая, определим функции выигрыша и вероятности перехода. В принципе, этого можно и не делать и работать в функции Беллмана напрямую с матрицами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(state_from, state_to, control):\n",
    "    if control == 1:\n",
    "        return R1[state_from, state_to]\n",
    "    elif control == 2:\n",
    "        return R2[state_from, state_to]\n",
    "    else:\n",
    "        raise Exception(f'Invalid control: {control}')\n",
    "        \n",
    "def state_transition_probability(state_from, state_to, control):\n",
    "    if control == 1:\n",
    "        return P1[state_from, state_to]\n",
    "    elif control == 2:\n",
    "        return P2[state_from, state_to]\n",
    "    else:\n",
    "        raise Exception(f'Invalid control: {control}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, вычисление функции Беллмана:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество лет (горизонт планирования)\n",
    "gardener_periods = 3\n",
    "\n",
    "# Таблица со значениями функции Беллмана\n",
    "# Ключом является пара (этап, состояние),\n",
    "# значением - пара (ожидаемый условный оптимальный выигрыш, условное оптимальное управление)\n",
    "gardener_W_table = {}\n",
    "\n",
    "def gardener_W(stage, state):\n",
    "    \"\"\"Вычисление функции Беллмана для задачи о садовнике.\n",
    "       \n",
    "       Возвращаемое значение: пара (ожидаемый условный оптимальный выигрыш, условное оптимальное управление).\"\"\"\n",
    "\n",
    "    # Условие выхода из рекурсии\n",
    "    if stage >= gardener_periods:\n",
    "        return (0, None)\n",
    "\n",
    "    # Перед перебором управлений проверим, нет ли еще значения для данных\n",
    "    # параметров в таблице:\n",
    "    if (stage, state) in gardener_W_table:\n",
    "        return gardener_W_table[(stage, state)]\n",
    "\n",
    "    # В соответствии с уравнением Беллмана для вероятностных задач, найдем \n",
    "    # ожидаемый условный оптимальный выигрыш, перебирая допустимые управления,\n",
    "    # и рассчитывая математическое ожидание их эффекта с учетом вероятностей переходов.\n",
    "    best_u = None\n",
    "    best_W = None\n",
    "    for control in [1, 2]:\n",
    "        # 1 - не вносить удобрения, 2 - вносить\n",
    "        \n",
    "        # Потенциально, при любом управлении мы можем перейти в любое\n",
    "        # состояние, поэтому здесь добавляется еще один цикл для оценки\n",
    "        # математического ожидания:\n",
    "        accumulated_sum = 0\n",
    "        for new_state in range(3):\n",
    "            accumulated_sum += state_transition_probability(state, new_state, control) * \\\n",
    "                                    (profit(state, new_state, control) + gardener_W(stage+1, new_state)[0])\n",
    "\n",
    "        # Если это управление лучше, чем наилучшее из известных к данному моменту - запомним его\n",
    "        if best_W is None or accumulated_sum > best_W:\n",
    "            best_W = accumulated_sum\n",
    "            best_u = control\n",
    "            \n",
    "    # Сохраним найденные значения в таблицу\n",
    "    gardener_W_table[(stage, state)] = (best_W, best_u)\n",
    "    return (best_W, best_u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим оценку ожидаемого условного оптимального выигрыша (если в начале процесса почва была в хорошем состоянии):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.7355, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_W(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, в этом случае наибольший ожидаемый выигрыш будет равен приблизительно 10.74 сотен долларов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Восстановление оптимальной стратегии\n",
    "\n",
    "В детерминированной модели мы могли полностью восстановить всю цепочку управлений (поскольку достоверно знали в каком состоянии система окажется в результате каждого из них). Здесь так сделать не получится - действительно, после того, как садовник применил оптимальное управление в первый год (осуществил подкормку), почва могла перейти в любое из трех состояний, и заранее сказать каким должно быть управление во второй год мы уже не можем.\n",
    "\n",
    "Поэтому для подобных задач результатом является полная *стратегия* оптимального управления, для каждого этапа и каждого состояния определяющая оптимальное действие (действие, приводящее к наибольшему ожидаемому выигрышу в заданной ситуации).\n",
    "\n",
    "Как и в предыдущем случае, данная стратегия уже содержится в словаре `gardener_W_table`, необходимо только извлечь ее оттуда. Для этого можно воспользоваться как самим словарем, так и функцией `gardener_W` которая все равно не будет рассчитывать значений, поскольку они уже содержатся в словаре. Итак, оптимальная стратегия для первого года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 1-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Применять удобрения\" -> 10.7355\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 7.922499999999999\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 4.22225\n"
     ]
    }
   ],
   "source": [
    "state_decoder = ['хорошее', 'удовлетворительное', 'плохое']\n",
    "control_decoder = [None, 'Не применять удобрения', 'Применять удобрения']\n",
    "\n",
    "print('Стратегия для 1-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(0, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично находится стратегия для 2-го года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 2-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Применять удобрения\" -> 8.19\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 5.61\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 2.125\n"
     ]
    }
   ],
   "source": [
    "print('Стратегия для 2-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(1, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И для третьего года:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия для 3-го года:\n",
      "  При состоянии почвы \"хорошее\": \"Не применять удобрения\" -> 5.300000000000001\n",
      "  При состоянии почвы \"удовлетворительное\": \"Применять удобрения\" -> 3.1\n",
      "  При состоянии почвы \"плохое\": \"Применять удобрения\" -> 0.40000000000000013\n"
     ]
    }
   ],
   "source": [
    "print('Стратегия для 3-го года:')\n",
    "for state in range(3):\n",
    "    expected_profit, control = gardener_W(2, state)\n",
    "    print(f'  При состоянии почвы \"{state_decoder[state]}\": \"{control_decoder[control]}\" -> {expected_profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование стратегии\n",
    "\n",
    "В качестве самопроверки может оказаться полезным промоделировать ту или иную стратегию управления и определить ожидаемый выигрыш от следования ей.\n",
    "\n",
    "Процесс может быть организован точно так же, как и вычисление функции Беллмана, только на каждом этапе нет необходимости осуществлять оптимизацию (выбор управления), требуется просто применять заданное управление.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внимание! Данная функция очень неэффективна без мемоизации. Лучше делать то же самое, но\n",
    "# в обратном порядке и без рекурсии\n",
    "def gardener_eval_strategy(strategy, state):\n",
    "    if len(strategy) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Управление, диктуемое текущей стратегией для данного состояния\n",
    "    control = strategy[0][state]\n",
    "    \n",
    "    accumulated_cost = 0\n",
    "    for new_state in range(3):\n",
    "        accumulated_cost += state_transition_probability(state, new_state, control) * \\\n",
    "                                    (profit(state, new_state, control) + gardener_eval_strategy(strategy[1:], new_state))\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим эффект \"скупой\" стратегии, при которой удобрения не вносятся ни при каких условиях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.212000000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_eval_strategy([[1, 1, 1],   # Управление для каждого из состояний на первом этапе\n",
    "                        [1, 1, 1],   # Управление для каждого из состояний на втором этапе\n",
    "                        [1, 1, 1]],  # ...\n",
    "                       0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, даже если мы начнем с почвы в \"хорошем\" состоянии, ожидаемый выигрыш будет около 8.2.\n",
    "\n",
    "Оценим эффект \"расточительной\" стратегии, при которой удобрения вносятся безусловно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.6425"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardener_eval_strategy([[2, 2, 2],\n",
    "                        [2, 2, 2],\n",
    "                        [2, 2, 2]],\n",
    "                       0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что при данной стратегии ожидаемый выигрыш оказывается очень близким к оптимальному. Это вполне ожидаемо, учитывая, что оптимальная стратегия почти совпадает с данной (единственное отличие заключается в том, что в оптимальной стратегии на третьем году в хорошую почву удобрения не вносятся)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матричная реализация\n",
    "\n",
    "Несмотря на то, что исходные данные задачи о садовнике были представлены в виде матриц (вероятностей и стоимостей перехода), реализация выше опиралась на наличие функций $w$ и $\\phi$, возвращающих характеристики перехода между одной парой состояний. Такое решение, с одной стороны, обладает достаточно высокой гибкостью (значения вероятностей и стоимостей могут вычисляться \"на лету\" по каким-то правилам), с другой, не позволяет использовать эффективные способы реализации, основанные на использовании векторизованных матричных операций.\n",
    "\n",
    "Рассмотрим более эффективный (хотя, возможно, и более сложный для понимания) способ реализации решения задачи нахождения оптимального управления в марковском процессе принятия решений с конечным горизонтом планирования.\n",
    "\n",
    "В первую очередь, представим данные в виде трехмерных матриц, первой размерностью которых будет управление, а второй и третьей - исходное и конечные состояния (для работы с многомерными матрицами используется библиотека `numpy`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2 , 0.5 , 0.3 ],\n",
       "        [0.  , 0.5 , 0.5 ],\n",
       "        [0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.3 , 0.6 , 0.1 ],\n",
       "        [0.1 , 0.6 , 0.3 ],\n",
       "        [0.05, 0.4 , 0.55]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([P1, P2])\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7,  6,  3],\n",
       "        [ 0,  5,  1],\n",
       "        [ 0,  0, -1]],\n",
       "\n",
       "       [[ 6,  5, -1],\n",
       "        [ 7,  4,  0],\n",
       "        [ 6,  3, -2]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([R1, R2])\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что при таком представлени удобнее нумеровать состояния с 0 (потому что элементы любой размерности массива в Python и numpy нумеруются именно с 0). То есть, управление \"не вносить удобрения\" теперь будет обозначаться 0, а \"вносить удобрения\" - 1. \n",
    "\n",
    "Так, вероятность перехода из состояния \"хорошая почва\" в состояние \"плохая почва\" при внесении удобрений будет вычисляться так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Управление 1 (вносим удобрения)\n",
    "# |  Исходное состояние 0 (хорошая почва)\n",
    "# |  |  Целевое состояние 2 (плохая почва)\n",
    "# |  |  |\n",
    "# v  v  v\n",
    "P[1, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем строить вычислительный процесс не рекурсивно (как раньше), а с последних этапов, используя метод \"обратной прогонки\". На каждом шаге необходимо для каждого исходного состояния найти такое управление, которое приведет к максимальному ожидаемому выигрышу, причем при \"обратной прогонке\" ожидаемый выигрыш для каждого состояния со следующего этапа уже известен. \n",
    "\n",
    "Попытаемся это реализовать. Итак, пусть есть вектор `W_next` () - ожидаемый выигрыш для каждого состояния, начиная со следующего этапа, матрица вероятностей переходов `Tp`, и матрица стоимостей переходов `Tc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_next = np.array([3, 2, 1])\n",
    "Tp = np.array([[0.5, 0.3, 0.2],\n",
    "               [0.1, 0.8, 0.1],\n",
    "               [0.7, 0.3, 0.0]])\n",
    "Tc = np.array([[7, 5, 4],\n",
    "               [3, 2, 1],\n",
    "               [8, 2, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда \"суммарная\" стоимость перехода (с учетом всех последствий), определяемая как стоимость перехода плюс ожидаемый выигрыш для того состояния, в которое осуществляется переход, может быть найдена следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  7,  5],\n",
       "       [ 6,  4,  2],\n",
       "       [11,  4,  6]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tc + W_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы имеем дело с такой особенностью большинства систем матричных вычислений, как broadcasting (распространение). Сложение матрицы 3х3 и вектора 1х3 по правилам линейной алгебры невозможно, но для того, чтобы сделать код более лаконичным, система как бы \"распространяет\", размножает матрицу меньшей размерности, делая операцию допустимой. В данном случае, вектор-строка размера (1,3) был добавлен к каждой строке матрицы `Tc` (то есть, \"размножен\" копированием строк до матрицы 3х3).\n",
    "\n",
    "Для получения математического ожидания выигрыша необходимо просуммировать произведение вероятностей событий на соответствующие выигрыши (для этого пригодится поэлементное умножение матриц и функция [numpy.sum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.1, 4. , 8.9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Tp * (Tc + W_next), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы получили оценку выигрыша для каждого состояния при известных матрицах переходов (данное выражение можно использовать для эффективной реализации оценки стратегии). Но для поиска оптимальной стратегии нужно уметь осуществлять выбор. То есть, на каждом этапе у нас есть не двумерные, а трехмерные матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2, 4.5, 0. ],\n",
       "       [6.9, 4.9, 1.9]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(P * (R + W_next), axis=-1)   # -1 означает, что суммирование производится по\n",
    "                                    # \"последней\" размерности, соответствующей целевому\n",
    "                                    # состоянию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У получившегося результата каждая строка соответствует ожидаемому выигрышу от определенного управления, а каждый столбец - (исходному) состоянию. Соответственно, для нахождения оптимального выигрыша нужно просто выбирать максимальный элемент в каждом столбце, а для нахождения оптимального управления - номер строки, соответствующей максимальному элементу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.2, 4.9, 1.9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sum(P * (R + W_next), axis=-1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.sum(P * (R + W_next), axis=-1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем все в одну функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предполагается, что матрица переходов одинакова для всех этапов\n",
    "# В принципе, это не обязательно и функцию можно легко усовершенствовать\n",
    "# Предполагается, что P и R имеют размерность 'количество управлений' х 'кол-во состояний' х 'кол-во состояний' \n",
    "def finite_horizon_mdp_solver(P, R, periods):\n",
    "    # Количество состояний\n",
    "    n_states = P.shape[1]\n",
    "    # Сформируем результирующие матрицы\n",
    "    profit = np.zeros((periods, n_states))                 # Условно оптимальные выигрыши\n",
    "    control = np.zeros((periods, n_states), dtype='int32') # Условно оптимальные управления\n",
    "    # Для последнего этапа выигрыш на \"следующем\" этапе\n",
    "    # должен быть равен нулю\n",
    "    profit_next = np.zeros((1, n_states))\n",
    "    for period in range(periods-1, -1, -1):\n",
    "        tmp = np.sum(P * (R + profit_next), axis=-1)\n",
    "        profit[period, :] = np.max(tmp, axis=0)\n",
    "        control[period, :] = np.argmax(tmp, axis=0)\n",
    "        profit_next = profit[period, :]\n",
    "    return profit, control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10.7355 ,  7.9225 ,  4.22225],\n",
       "        [ 8.19   ,  5.61   ,  2.125  ],\n",
       "        [ 5.3    ,  3.1    ,  0.4    ]]),\n",
       " array([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 1, 1]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finite_horizon_mdp_solver(P, R, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним, действительно ли это оказывается быстрее, чем исходная реализация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 542 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4515.99034760127, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gardener_periods = 2000\n",
    "gardener_W_table = {}\n",
    "gardener_W(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 98.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4515.99034760127, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = finite_horizon_mdp_solver(P, R, 2000)\n",
    "_[0][0,0], _[1][1,0] + 1   # +1, потому что мы изменили способ кодирования управлений с (1,2) на (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При данном количестве состояний (3) разница в скорости выполнения этих двух реализаций оказалась приблизительно в три раза. Скорее всего, при б*о*льшем количестве состояний разрыв окажется еще более существенным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сведение задачи к марковскому процессу принятия решений\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
